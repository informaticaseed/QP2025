# üè† Aula 12: KNN (K-Nearest Neighbors) - Aprendendo com os Vizinhos

## ‚è±Ô∏è Cronograma da Aula (50 min)
* **(10 min) Introdu√ß√£o:** O conceito de vizinhan√ßa e similaridade
* **(15 min) Demonstra√ß√£o:** Classifica√ß√£o por vota√ß√£o dos vizinhos
* **(20 min) Pr√°tica:** Implementa√ß√£o do KNN em Python
* **(5 min) Desafio:** Classificar novos pontos com KNN

## üéØ Objetivo da Aula
Entender como o algoritmo KNN classifica novos dados com base na similaridade com exemplos conhecidos.

## üß† Conceito Simplificado
KNN √© como perguntar a opini√£o das pessoas mais parecidas com voc√™ para tomar uma decis√£o.

## üèôÔ∏è Analogia do Restaurante
**"Escolhendo um Restaurante"**:
- Voc√™ quer saber se vai gostar de um novo restaurante
- Pergunta a opini√£o de 5 amigos com gostos parecidos aos seus
- Se a maioria gostou, voc√™ provavelmente tamb√©m vai gostar
- O KNN funciona assim: ele consulta os "vizinhos mais pr√≥ximos" para decidir

## üíª Vamos Praticar no Colab!

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from matplotlib.colors import ListedColormap

# DADOS: Caracter√≠sticas de frutas [peso em gramas, n√≠vel de do√ßura (0-10)]
frutas = np.array([
    [150, 7], [170, 8], [140, 6], [130, 7], [160, 8],  # Ma√ß√£s
    [60, 9], [80, 10], [70, 9], [50, 8], [75, 10],    # Morangos
    [250, 3], [300, 4], [280, 3], [270, 4], [260, 2]   # Abacates
])
# R√≥tulos das frutas: 0 = Ma√ß√£, 1 = Morango, 2 = Abacate
rotulos = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])
nomes_frutas = ['Ma√ß√£', 'Morango', 'Abacate']

# NORMALIZAR os dados
scaler = StandardScaler()
frutas_normalizadas = scaler.fit_transform(frutas)

# VISUALIZAR os dados originais
plt.figure(figsize=(12, 8))
cores = ['red', 'green', 'blue']
for i in range(3):  # Para cada tipo de fruta
    indices = np.where(rotulos == i)
    plt.scatter(
        frutas[indices, 0], frutas[indices, 1],
        c=cores[i], label=nomes_frutas[i],
        s=100, edgecolor='k'
    )

plt.title('Caracter√≠sticas das Frutas', fontsize=16)
plt.xlabel('Peso (gramas)', fontsize=14)
plt.ylabel('N√≠vel de Do√ßura (0-10)', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.show()

# CRIAR E TREINAR o modelo KNN
k = 3  # N√∫mero de vizinhos a considerar
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(frutas_normalizadas, rotulos)

# VISUALIZAR as fronteiras de decis√£o
def plot_decision_boundaries(X, y, model, scaler):
    h = 0.02  # Tamanho da grade
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    
    # Escalonar os pontos da grade
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    grid_points_scaled = scaler.transform(grid_points)
    
    # Prever as classes para cada ponto da grade
    Z = model.predict(grid_points_scaled)
    Z = Z.reshape(xx.shape)
    
    # Criar um mapa de cores personalizado
    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
    
    # Plotar as regi√µes coloridas
    plt.figure(figsize=(12, 8))
    plt.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)
    
    # Plotar os pontos de treinamento
    for i in range(3):
        indices = np.where(y == i)
        plt.scatter(
            X[indices, 0], X[indices, 1],
            c=cmap_bold.colors[i], 
            label=nomes_frutas[i],
            s=100, edgecolor='k'
        )
    
    plt.title(f'Fronteiras de Decis√£o do KNN (k={k})', fontsize=16)
    plt.xlabel('Peso (gramas)', fontsize=14)
    plt.ylabel('N√≠vel de Do√ßura (0-10)', fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.show()

# Mostrar as fronteiras de decis√£o
plot_decision_boundaries(frutas, rotulos, knn, scaler)

# TESTAR com uma nova fruta
nova_fruta = np.array([[200, 8]])  # Uma fruta pesando 200g com do√ßura 8
nova_fruta_normalizada = scaler.transform(nova_fruta)
previsao = knn.predict(nova_fruta_normalizada)
probabilidades = knn.predict_proba(nova_fruta_normalizada)

print(f"Nova fruta: Peso = 200g, Do√ßura = 8")
print(f"O KNN classificou como: {nomes_frutas[previsao[0]]}")
print(f"Probabilidades: Ma√ß√£: {probabilidades[0][0]:.2f}, Morango: {probabilidades[0][1]:.2f}, Abacate: {probabilidades[0][2]:.2f}")

# VISUALIZAR a nova fruta no gr√°fico
plt.figure(figsize=(12, 8))
for i in range(3):
    indices = np.where(rotulos == i)
    plt.scatter(
        frutas[indices, 0], frutas[indices, 1],
        c=cores[i], label=nomes_frutas[i],
        s=100, edgecolor='k'
    )

# Destacar a nova fruta
plt.scatter(
    nova_fruta[:, 0], nova_fruta[:, 1],
    c='yellow', marker='*', s=300, edgecolor='k',
    label='Nova Fruta'
)

# Mostrar os k vizinhos mais pr√≥ximos
# Encontrar os √≠ndices dos k vizinhos mais pr√≥ximos
vizinhos = knn.kneighbors(nova_fruta_normalizada, return_distance=False)[0]
for vizinho in vizinhos:
    plt.plot(
        [nova_fruta[0, 0], frutas[vizinho, 0]],
        [nova_fruta[0, 1], frutas[vizinho, 1]],
        'k--', alpha=0.6
    )
    plt.scatter(
        frutas[vizinho, 0], frutas[vizinho, 1],
        s=150, edgecolor='k', facecolor='none', linewidth=2
    )

plt.title('Nova Fruta e seus Vizinhos Mais Pr√≥ximos', fontsize=16)
plt.xlabel('Peso (gramas)', fontsize=14)
plt.ylabel('N√≠vel de Do√ßura (0-10)', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.show()
```

## üß© Como Funciona o KNN

1. **Calcula a dist√¢ncia** entre o novo ponto e todos os pontos conhecidos
2. **Seleciona os K vizinhos mais pr√≥ximos** (com menor dist√¢ncia)
3. **Para classifica√ß√£o**: escolhe a classe mais comum entre os vizinhos
4. **Para regress√£o**: calcula a m√©dia dos valores dos vizinhos

## üìè Tipos de Dist√¢ncia
- **Euclidiana**: reta entre dois pontos (a mais comum)
- **Manhattan**: dist√¢ncia seguindo apenas linhas horizontais e verticais
- **Minkowski**: generaliza√ß√£o que inclui as duas anteriores

## üìä Aplica√ß√µes no Mundo Real
- Sistemas de recomenda√ß√£o ("quem gostou disto tamb√©m gostou daquilo")
- Reconhecimento facial
- Diagn√≥stico m√©dico baseado em casos similares
- Previs√£o de pre√ßos imobili√°rios
- Classifica√ß√£o de documentos

## üìù Exerc√≠cio Pr√°tico
Use o KNN para classificar filmes em "A√ß√£o", "Com√©dia" ou "Drama" com base em:
- N√∫mero de cenas de a√ß√£o (0-10)
- N√∫mero de piadas (0-10)

```python
filmes = np.array([
    [8, 2], [9, 1], [7, 3], [9, 2], [8, 1],  # A√ß√£o
    [2, 9], [3, 8], [1, 10], [2, 8], [3, 9],  # Com√©dia
    [4, 2], [5, 3], [3, 1], [4, 1], [5, 2]   # Drama
])
generos = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])
```

Perguntas:
1. Como seria classificado um filme com 6 cenas de a√ß√£o e 6 piadas?
2. Como o valor de K afeta a classifica√ß√£o?
3. Quais s√£o as vantagens e desvantagens do KNN?

## üí° Vantagens do KNN
1. **Simples de entender e implementar**
2. **N√£o faz suposi√ß√µes sobre a distribui√ß√£o dos dados**
3. **Pode capturar padr√µes complexos**
4. **Funciona bem para classifica√ß√£o e regress√£o**

## ‚ö†Ô∏è Limita√ß√µes
- **Computacionalmente intensivo** para grandes conjuntos de dados
- **Sens√≠vel a caracter√≠sticas irrelevantes**
- **Requer normaliza√ß√£o** das caracter√≠sticas
- **Desempenho cai em espa√ßos de alta dimens√£o**

## üéØ Aplica√ß√£o no Projeto Final
Suas ideias para KNN podem incluir:
- Sistema de recomenda√ß√£o de filmes
- Classifica√ß√£o de munic√≠pios em grupos de desenvolvimento similares
- Previs√£o de comportamento eleitoral com base em regi√µes similares
- Identifica√ß√£o de padr√µes de criminalidade similares entre estados

---